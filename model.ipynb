{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier as model\n",
    "import pickle\n",
    "import math\n",
    "import itertools\n",
    "import joblib\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import seaborn as sns\n",
    "import gdal\n",
    "from numpy import hstack\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD \n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPool1D, Conv1D, MaxPooling1D, Embedding,LSTM, Bidirectional, Dropout, Flatten,BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import Parallel, delayed\n",
    "from functools import reduce\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'F:/tif data/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Landslides are', round(df['label'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
    "print('No Landslides are', round(df['label'].value_counts()[0]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.sample(frac=1, random_state=42).drop(['x','y'],axis=1,inplace=True)\n",
    "new_df=ndf.copy()\n",
    "undersample = OneSidedSelection(n_neighbors=1, n_seeds_S=200,n_jobs=-1)\n",
    "X, y = undersample.fit_resample(new_df.drop('label',axis=1), new_df['label'])\n",
    "X['label']=y\n",
    "print('Landslides are', round(X['label'].value_counts()[1]/len(X) * 100,2), '% of the dataset')\n",
    "print('No Landslides are', round(X['label'].value_counts()[0]/len(X) * 100,2), '% of the dataset')\n",
    "X.to_csv('./train_OSS.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('o', over)])\n",
    "X_train, y_train = pipeline.fit_resample(train.drop('label',axis=1), train['label'])\n",
    "print('Resample dataset shape', Counter(y_train))\n",
    "X_train['label']=y_train\n",
    "X_train.to_csv(r'F:/tif data/training_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'F:/python_nb/training_data.csv')\n",
    "test=pd.read_csv(r'F:/tif data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(44,20),dpi=300)# Entire DataFrame\n",
    "ax = plt.axes()\n",
    "X_train=train.drop(['x','y','label'],axis=1)\n",
    "d={'road':'Road','aspec':'Aspect','sand':'Sand','silt':'Silt','river':'River'}\n",
    "X_train.rename(d,axis=1,inplace=True)\n",
    "corr = X_train.corr()\n",
    "\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax)\n",
    "ax.set_title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.savefig('corr.jpg',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'F:/tif data/test.csv')\n",
    "X_train=train.drop(['x','y','label'],axis=1)\n",
    "y_train=train['label']\n",
    "\n",
    "X_test=test.drop(['x','y','label'],axis=1)\n",
    "y_test=test['label']\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "min_max_scaler = MinMaxScaler().fit(X_train) # Fit On Training Data\n",
    "X_train = min_max_scaler.transform(X_train) # Transform On Training Data\n",
    "X_test= min_max_scaler.transform(X_test) # Transform On Validation Data\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'road':'Road','aspec':'Aspect','sand':'Sand','silt':'Silt','river':'River'}\n",
    "test.rename(d,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,8))\n",
    "ax=plt.axes()\n",
    "feature_scores = pd.Series(model.feature_importances_, index=test.drop(['x','y','label'],axis=1).columns).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_scores, y=feature_scores.index,color='grey')\n",
    "plt.xlabel('Feature Importance Score',fontsize=20)\n",
    "plt.ylabel('Landslide Conditioning Factors',fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=18)\n",
    "plt.savefig(r'F:\\paper1img/feature_importance.jpg',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # Sequence level Input Model\n",
    "    \n",
    "    Sequence_Input_tensor = Input(shape=(X_train.shape[1],),name='Sequence_input_tensor')\n",
    "\n",
    "\n",
    "    x1=layers.Dense(units=16,\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=keras.regularizers.l1_l2(l1=0,l2=0.01), \n",
    "                      bias_regularizer=keras.regularizers.l1_l2(l1=0,l2=0.01),\n",
    "                      kernel_initializer= tf.keras.initializers.he_normal(seed=700),\n",
    "                      bias_initializer=tf.keras.initializers.Zeros()   )(Sequence_Input_tensor) \n",
    "    \n",
    "\n",
    "    \n",
    "    x1=layers.Dense(units=8,\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=keras.regularizers.l1_l2(l1=0,l2=0.01), \n",
    "                      bias_regularizer=keras.regularizers.l1_l2(l1=0,l2=0.01),\n",
    "                      kernel_initializer= tf.keras.initializers.he_normal(seed=700),\n",
    "                      bias_initializer=tf.keras.initializers.Zeros()   )(x1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x1=layers.Dense(units=4,\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=keras.regularizers.l1_l2(l1=0,l2=0.01), \n",
    "                      bias_regularizer=keras.regularizers.l1_l2(l1=0,l2=0.01),\n",
    "                      kernel_initializer= tf.keras.initializers.he_normal(seed=700),\n",
    "                      bias_initializer=tf.keras.initializers.Zeros()   )(x1)\n",
    "    \n",
    "    x1=layers.BatchNormalization()(x1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output_tensor = layers.Dense(1,\n",
    "                          activation='sigmoid',\n",
    "                          kernel_initializer= tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=700),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros()) (x1)\n",
    "    \n",
    "\n",
    "    Basic_Model = Model(inputs=[Sequence_Input_tensor], outputs=output_tensor)\n",
    "\n",
    "    \n",
    "    Basic_Model.compile(optimizer=Adam(10**-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Basic_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 30\n",
    "Training_scores= list()\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping('val_loss',patience=40,restore_best_weights=True)]  # Callbacks\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model_1=build_model()\n",
    "    history_1=model_1.fit(X_train,y_train, batch_size=batch_size, epochs=epochs,shuffle=True, verbose=2,validation_data=(X_test,y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(probability_array):\n",
    "    for i in range(len(probability_array)):\n",
    "        if (probability_array[i]>0.5):\n",
    "            probability_array[i]=1\n",
    "        else:\n",
    "            probability_array[i]=0\n",
    "    return (probability_array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def accuracy_check(pred,AY_VALIDATE):\n",
    "    abc=confusion_matrix(AY_VALIDATE,pred,normalize='true')\n",
    "\n",
    "    TN=abc[0][0]\n",
    "    FP=abc[0][1]\n",
    "    FN=abc[1][0]\n",
    "    TP=abc[1][1]\n",
    "\n",
    "    PS=FN+TP\n",
    "    NS=TN+FP\n",
    "    Accuracy           = ((TP+TN)/(PS+NS))*100\n",
    "    Sensitivity        = (TP/PS)*100\n",
    "    Precision          = (TP/(FP+TP))*100\n",
    "    F1                 = (2*Sensitivity*Precision)/(Sensitivity+Precision )\n",
    "    Speciﬁcity         = (TN/NS)*100\n",
    "    BAC                = (Sensitivity + Speciﬁcity)/2\n",
    "    MCC                = metrics.matthews_corrcoef(y_test,(model.predict(X_test)))\n",
    "    print('Accuracy:' ,Accuracy)\n",
    "    print('Sensitivity:', Sensitivity)\n",
    "    print('Precision:' ,Precision)\n",
    "    print('matrix',abc)  \n",
    "    print(confusion_matrix(AY_VALIDATE,pred))\n",
    "    MCC=(TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    print('MCC:',MCC)\n",
    "    cmap=plt.cm.Greens\n",
    "    title='Confusion matrix'\n",
    "    cm=abc\n",
    "    classes = ['No-Landslide', 'Landslide']\n",
    "    normalize=True\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Label the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), \n",
    "             fontsize = 20,\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    # plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(('RF', RF(n_estimators=n_estimators,max_features='sqrt',n_jobs=-1)))\n",
    "\tmodels.append(('ANN', ANN())\n",
    "\tmodels.append(('svm', SVC(kernel=kernel,C=C,probability=True)))\n",
    "\treturn models\n",
    "\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "\t# fit all models on the training set and predict on hold out set\n",
    "\tmeta_X = list()\n",
    "\tfor name, model in models:\n",
    "\t\t# fit in training set\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\t# predict on hold out set\n",
    "\t\tyhat = model.predict_proba(X_val)[:,1]\n",
    "\t\t# reshape predictions into a matrix with one column\n",
    "\t\tyhat = yhat.reshape(len(yhat), 1)\n",
    "\t\t# store predictions as input for blending\n",
    "\t\tmeta_X.append(yhat)\n",
    "\t# create 2d array from predictions, each set is an input feature\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# define blending model\n",
    "\tblender = LogisticRegression()\n",
    "\t# fit on predictions from base models\n",
    "\tblender.fit(meta_X, y_val)\n",
    "\treturn blender\n",
    "\n",
    "def predict_ensemble(models, blender, X_test):\n",
    "\t# make predictions with base models\n",
    "\tmeta_X = list()\n",
    "\tfor name, model in models:\n",
    "\t\t# predict with base model\n",
    "\t\tyhat = model.predict_proba(X_test)[:,1]\n",
    "\t\t# reshape predictions into a matrix with one column\n",
    "\t\tyhat = yhat.reshape(len(yhat), 1)\n",
    "\t\t# store prediction\n",
    "\t\tmeta_X.append(yhat)\n",
    "\t# create 2d array from predictions, each set is an input feature\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn blender.predict_proba(meta_X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_TRAIN, Y_TRAIN, test_size=0.33, random_state=1)\n",
    "X_test=X_TEST\n",
    "Y_test=Y_TEST\n",
    "\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "# make predictions on test set\n",
    "yhat = predict_ensemble(models, blender, X_test)\n",
    "# evaluate predictions\n",
    "score = accuracy_score(Y_test, yhat)\n",
    "print('Blending Accuracy: %.3f' % (score*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
